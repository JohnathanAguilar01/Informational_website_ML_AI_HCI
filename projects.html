<!DOCTYPE HTML>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Information website on ML + AI with regards to HCI</title>
<link href="css/styles.css" rel="stylesheet" type="text/css">
</head>
<body>
<header role="banner">
  <nav role="navigation">
    <h1>Information website</h1>
    <ul>
      <li><a href="index.html" title="Home">Home</a></li>
      <li><a href="research.html" title="Research Domains">Research Domains</a></li>
      <li><a href="projects.html" title="Projects">Projects</a></li>
      <li><a href="resources.html" title="Resources">Resources</a></li>
    </ul>
  </nav>
  <h2>Human-Centered AI (HCAI) Projects</h2>
</header>
<main role="main">
  <article>
    <section>
      <p>
        The convergence of Human-Computer Interaction (HCI) and Artificial Intelligence (AI) is shaping the future of human-technology
        engagement. Research in this interdisciplinary domain primarily addresses the design of intelligent systems that support human-AI
        collaboration, with an emphasis on adaptability, intuitive interaction, and explainability. Current investigations extend beyond
        task automation to focus on improving decision-making, creativity, and productivity. By integrating AI capabilities in perception,
        reasoning, and learning with HCI principles of usability and user experience, researchers seek to develop technologies that are 
        centered on human needs. At the University of California, Los Angeles (UCLA), the HCI Research Lab (HiLab) focuses on adaptive
        interfaces, interactive machine learning, and innovative collaborative human-AI systems.
      </p> 
    </section>

    <section>
      <h3>Watch Your Mouth: Silent Speech Recognition with Depth Sensing</h3>
      <h4>Abstract</h4>
      <p>
        Silent speech recognition is a promising technology that decodes
        human speech without requiring audio signals, enabling private
        human-computer interactions. In this paper, we propose Watch
        Your Mouth, a novel method that leverages depth sensing to enable
        accurate silent speech recognition. By leveraging depth information,
        our method provides unique resilience against environmental
        factors such as variations in lighting and device orientations, while
        further addressing privacy concerns by eliminating the need for
        sensitive RGB data. We started by building a deep-learning model
        that locates lips using depth data. We then designed a deep learning
        pipeline to effciently learn from point clouds and translate
        lip movements into commands and sentences. We evaluated our
        technique and found it effective across diverse sensor locations:
        On-Head, On-Wrist, and In-Environment. Watch Your Mouth out-
        performed the state-of-the-art RGB-based method, demonstrating
        its potential as an accurate and reliable input technique.
      <p>
      <a  href="https://hilab.dev/research/WatchYourMouth/WatchYourMouth.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/speach_thumbnail.webm" type="video/webm">
        your browser does not support the video.
      </video>
    </section>
    <section>
      <h3>WheelPose</h3>
      <h4>Abstract</h4>
      <p>
        Existing pose estimation models perform poorly on wheelchair
        users due to a lack of representation in training data. We present
        a data synthesis pipeline to address this disparity in data collection
        and subsequently improve pose estimation performance for
        wheelchair users. Our confgurable pipeline generates synthetic
        data of wheelchair users using motion capture data and motion
        generation outputs simulated in the Unity game engine. We validated
        our pipeline by conducting a human evaluation, investigating
        perceived realism, diversity, and an AI performance evaluation on
        a set of synthetic datasets from our pipeline that synthesized differnt
        backgrounds, models, and postures. We found our generated
        datasets were perceived as realistic by human evaluators, had more
        diversity than existing image datasets, and had improved person
      </p>
      <a href="https://hilab.dev/research/WheelPose/WheelPose.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/wheelpose_thumbnail.webm" type="video/webm">
        your browser does not support the video.
      </video>
    </section>
    <section>
      <h3>FingerSwitches: UI Mobility Control in XR</h3>
      <h4>Abstract</h4>
      <p>
        Extended reality (XR) has the potential for seamless user interface
        (UI) transitions across people, objects, and environments. However,
        the design space, applications, and common practices of 3D UI transitions
        remain underexplored. To address this gap, we conducted
        a need-finding study with 11 participants, identifying and distilling a
        taxonomy based on three types of UI placements — affixed
        to static, dynamic, or self entities. We further surveyed 113 commercial
        applications to understand the common practices of 3D
        UI mobility control, where only 6.2% of these applications allowed
        users to transition UI between entities. In response, we built 
        interaction prototypes to facilitate UI transitions between entities.
        We report on results from a qualitative user study (N=14) on 3D
        UI mobility control using our FingerSwitches technique, which
        suggests that perceived usefulness is affected by types of entities
        and environments. We aspire to tackle a vital need in UI mobility
        within XR.
      </p>
      <a href="https://hilab.dev/research/FingerSwitches/FingerSwitches.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/finger_thumbnail.webm" type="video/webm">
        your browser does not support the video.
      </video>
    </section>
    <section>
      <h3>Embodied Exploration: Remote Accessibility Assessment in VR</h3>
      <h4>Abstract</h4>
      <p>        
        Acquiring accessibility information about unfamiliar places in advance
        is essential for wheelchair users to make better decisions
        about physical visits. Today’s assessment approaches such as phone
        calls, photos/videos, or 360◦ virtual tours often fall short of
        providing the specific accessibility details needed for individual differences.
        For example, they may not reveal crucial information like
        whether the legroom underneath a table is spacious enough or if the
        spatial configuration of an appliance is convenient for wheelchair
        users. In response, we present Embodied Exploration, a Virtual Reality
        (VR) technique to deliver the experience of a physical visit
        while keeping the convenience of remote assessment. Embodied
        Exploration allows wheelchair users to explore high-fidelity digital
        replicas of physical environments with themselves embodied by
        avatars, leveraging the increasingly affordable VR headsets. With
        a preliminary exploratory study, we investigated the needs and
        iteratively refined our techniques. Through a real-world user study
        with six wheelchair users, we found Embodied Exploration is able
        to facilitate remote and accurate accessibility assessment. We also
        discuss design implications for embodiment, safety, and practicality.
      </p>
      <a href="https://hilab.dev/research/EmbodiedExploration/EmbodiedExploration.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/embodied_thumbnail.webm" type="video/webm">
        your browser does not support the video.
      </video>
    </section>
    <section>
      <h3>TextureSight</h3>
      <h4>Abstract:</h4>
      <p>        
        Objects engaged by users’ hands contain rich contextual information for their strong correlation with user activities. Tools
        such as toothbrushes and wipes indicate cleansing and sanitation, while mice and keyboards imply work. Much research
        has been endeavored to sense hand-engaged objects to supply wearables with implicit interactions or ambient computing
        with personal informatics. We propose TextureSight, a smart-ring sensor that detects hand-engaged objects by detecting
        their distinctive surface textures using laser speckle imaging on a ring form factor. We conducted a two-day experience
        sampling study to investigate the unicity and repeatability of the object-texture combinations across routine objects. We
        grounded our sensing with a theoretical model and simulations, powered it with state-of-the-art deep neural net techniques,
        and evaluated it with a user study. TextureSight constitutes a valuable addition to the literature for its capability to sense
        passive objects without emission of EMI or vibration and its elimination of lens for preserving user privacy, leading to a new,
        practical method for activity recognition and context-aware computing.
      </p>
      <a href="https://hilab.dev/research/TextureSight/TextureSight.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/texture_thumbnail.webm" type="video/webm">
        your browser does not support the video.
      </video>
    </section>
    <section>
      <h3>T2IRay: Thumb-to-Index Indirect Pointing</h3>
      <h4>Abstract:</h4>
      <p>
        Free-hand interactions have been widely deployed for AR/VR interfaces
        to promote a natural and seamless interaction experience.
        Among various types of hand interactions, microgestures are still
        limited in supporting discrete inputs and in lacking a continuous
        interaction theme. To this end, we propose a new pointing 
        technique, T2IRay, which enables continuous indirect pointing through
        microgestures for continuous spatial input. We employ our own
        local coordinate system based on the thumb-to-index finger 
        relationship to map the computed raycasting direction for indirect
        pointing in a virtual environment. Furthermore, we examine 
        various mapping methodologies and collect thumb-click behaviors to
        formulate thumb-to-index microgesture design guidelines to foster
        continuous, reliable input. We evaluate the design parameters for
        mapping indirect pointing with acceptable speed, depth, and range.
        We collect and analyze the characteristics of click behaviors for 
        future implementation. Our research demonstrates the potential and
        practicality of free-hand micro-finger input methods for advancing
        future interaction paradigms.
      </p>
      <a href="https://hilab.dev/research/T2IRay/T2IRay.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/thumb_thumbnail.mp4" type="video/mp4">
        your browser does not support the video.
      </video>
    </section>
    <section>
      <h3>Headar</h3>
      <h4>Abstract:</h4>
      <p>
        Nod and shake of one’s head are intuitive and universal gestures in communication. As smartwatches become increasingly
        intelligent through advances in user activity sensing technologies, many use scenarios of smartwatches demand quick
        responses from users in confirmation dialogs, to accept or dismiss proposed actions. Such proposed actions include making
        emergency calls, taking service recommendations, and starting or stopping exercise timers. Head gestures in these scenarios
        could be preferable to touch interactions for being hands-free and easy to perform. We propose Headar to recognize these
        gestures on smartwatches using wearable millimeter wave sensing. We first surveyed head gestures to understand how
        they are performed in conversational settings. We then investigated positions and orientations to which users raise their
        smartwatches. Insights from these studies guided the implementation of Headar. Additionally, we conducted modeling and
        simulation to verify our sensing principle. We developed a real-time sensing and inference pipeline using contemporary
        deep learning techniques, and proved the feasibility of our proposed approach with a user study (n=15) and a live test (n=8).
        Our evaluation yielded an average accuracy of 84.0% in the user study across 9 classes including nod and shake as well
        as seven other signals – still, speech, touch interaction, and four non-gestural head motions (i.e., head up, left, right, and
        down). Furthermore, we obtained an accuracy of 72.6% in the live test which reveals rich insights into the performance of our
        approach in various realistic conditions. 
      </p>
      <a href="https://hilab.dev/research/Headar/Headar.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/headar_thumbnail.webm" type="video/webm">
        your browser does not support the video.
      </video>
    </section>
    <section>
      <h3>CubeSense++ (Smart Environment Sensing)</h3>
      <h4>Abstract:</h4>
      <p>        
        Smart environment sensing provides valuable contextual 
        information by detecting occurrences of events such as human activities
        and changes of object status, enabling computers to collect
        personal and environmental informatics to perform timely responses
        to user’s needs. Conventional approaches either rely on tags that 
        require batteries and frequent maintenance, or have limited detection
        capabilities bounded by only a few coarsely predefined activities.
        In response, this paper explores corner reflector mechanisms that
        encode user interactions with everyday objects into structured
        responses to millimeter wave radar, which has the potential for
        integration into smart environment entities such as speakers, light
        bulbs, thermostats, and autonomous vehicles. We presented the
        design space of 3D printed reflectors and gear mechanisms, which
        are low-cost, durable, battery-free, and can retrofit to a wide array
        of objects. These mechanisms convert the kinetic energy from user
        interactions into rotational motions of corner reflectors which we
        computationally designed with a genetic algorithm. We built an
        end-to-end radar detection pipeline to recognize fine-grained activity
        information such as state, direction, rate, count, and usage
        based on the characteristics of radar responses. We conducted studies
        for multiple instrumented objects in both indoor and outdoor
        environments, with promising results demonstrating the feasibility
        of the proposed approach.
      </p>
      <a href="https://hilab.dev/research/CubeSensePlusPlus/CubeSensePlusPlus.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/cube_thumbnail.webm" type="video/webm">
        your browser does not support the video.
      </video>
    </section>
    <section>
      <h3>Interaction Harvesting / Interaction-Powered Widgets</h3>
      <h4>Abstract:</h4>
      <p>
        Whenever a user interacts with a device, mechanical work is performed to actuate the user interface elements; the resulting
        energy is typically wasted, dissipated as sound and heat. Previous work has shown that many devices can be powered entirely
        from this otherwise wasted user interface energy. For these devices, wires and batteries, along with the related hassles of
        replacement and charging, become unnecessary and onerous. So far, these works have been restricted to proof-of-concept
        demonstrations; a specific bespoke harvesting and sensing circuit is constructed for the application at hand. The challenge of
        harvesting energy while simultaneously sensing fine-grained input signals from diverse modalities makes prototyping new
        devices difficult. To fill this gap, we present a hardware toolkit which provides a common electrical interface for harvesting
        energy from user interface elements. This facilitates exploring the composability, utility, and breadth of enabled applications
        of interaction-powered smart devices. We design a set of "energy as input" harvesting circuits, a standard connective interface
        with 3D printed enclosures, and software libraries to enable the exploration of devices where the user action generates the
        energy needed to perform the device’s primary function. This exploration culminated in a demonstration campaign where we
        prototype several exemplar popular toys and gadgets, including battery-free Bop-It— a popular 90s rhythm game, an electronic
        Etch-a-sketch, a "Simon-Says"-style memory game, and a service rating device. We run exploratory user studies to understand
        how generativity, creativity, and composability are hampered or facilitated by these devices. These demonstrations, user study
        takeaways, and the toolkit itself provide a foundation for building interactive and user-focused gadgets whose usability is not
        affected by battery charge and whose service lifetime is not limited by battery wear.
      </p>
      <a href="https://hilab.dev/research/InteractionHarvesting/InteractionHarvesting.pdf">Paper</a>
      <video autoplay loop muted playsinline width="640">
        <source src="images/simon_thumbnail.webm" type="video/webm">
        your browser does not support the video.
      </video>
    </section>
  </article>
</main>
<footer role="contentinfo">
  <div class="social-links">
    <p>Johnathan Aguilar's Conatct: </p>
    <!-- GitHub -->
    <a href="https://github.com/JohnathanAguilar01" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
      <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" xmlns="http://www.w3.org/2000/svg">
        <path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 
        0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 
        2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 
        2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 
        5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 
        8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 
        239.2 12.8 2.3 17.3-5.6 17.3-12.1 
        0-6.2-.3-40.4-.3-61.4 0 0-70 
        15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 
        0 0-22.9-15.7 1.6-15.4 0 0 24.9 
        2 38.6 25.8 21.9 38.6 58.6 27.5 
        72.9 20.9 2.3-16 8.8-27.1 
        16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 
        0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 
        2.6-67.9 20.9-6.5 69 27 69 27 
        20-5.6 41.5-8.5 62.8-8.5s42.8 
        2.9 62.8 8.5c0 0 48.1-33.6 
        69-27 13.7 34.7 5.2 61.4 
        2.6 67.9 16 17.7 25.8 31.5 
        25.8 58.9 0 96.5-58.9 104.2-114.8 
        110.5 9.2 7.9 17 22.9 
        17 46.4 0 33.7-.3 75.4-.3 
        83.6 0 6.5 4.6 14.4 
        17.3 12.1C428.2 457.8 496 362.9 
        496 252 496 113.3 383.5 8 
        244.8 8zM97.2 352.9c-1.3 1-1 
        3.3.7 5.2 1.6 1.6 3.9 2.3 
        5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 
        1.3.3 2.9 2.3 3.9 1.6 1 
        3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 
        35.6c-1.6 1.3-1 4.3 1.3 
        6.2 2.3 2.3 5.2 2.6 
        6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 
        1-1.6 3.6 0 5.9 1.6 2.3 
        4.3 3.3 5.6 2.3 1.6-1.3 
        1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path>
      </svg>
      GitHub
    </a>

    <!-- LinkedIn -->
    <a href="https://linkedin.com/in/johnathangaguilar/" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn">
      <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" xmlns="http://www.w3.org/2000/svg">
        <path d="M416 32H31.9C14.3 32 0 46.5 
        0 64.3v383.4C0 465.5 14.3 
        480 31.9 480H416c17.6 0 
        32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 
        416H69V202.2h66.5V416zm-33.2-243c-21.3 
        0-38.5-17.3-38.5-38.5S80.9 
        96 102.2 96c21.2 0 38.5 
        17.3 38.5 38.5 0 21.3-17.2 
        38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 
        0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 
        30.6-34.5 62.9-34.5 67.2 
        0 79.7 44.3 79.7 101.9V416z"></path>
      </svg>
      LinkedIn
    </a>
  </div>
</footer>
</body>
</html>
